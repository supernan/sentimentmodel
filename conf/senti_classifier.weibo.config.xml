<?xml version="1.0" encoding="utf-8"?>
<number_senti_classifiers>
	<number_senti_classifier>
		<number_weight type=float>1.5</number_weight>
		<identifier>centroid_senti_classifier</identifier>
		<param>
			<senti_centroid_files>
                <senti_centroid_file>../data/model/weibo.senti.liblinear.model</senti_centroid_file>
			</senti_centroid_files>
			<is_ensure_nonnegative>true</is_ensure_nonnegative>
			<norm_method>l2norm</norm_method>
			<doc_vec_builder>
				<identifier>standard_doc_vec_builder</identifier>
				<param>
					<word_weighting_method>tf-idf</word_weighting_method>
					<normalize_method>l2norm</normalize_method>
                    <vocabulary_file>../data/vocabulary/vocabulary.weibo.ngram.dat</vocabulary_file>
				</param>
			</doc_vec_builder>
			<text_analyzer>
				<is_thread_safe>true</is_thread_safe>
				<analyzer>
					<encode>utf8</encode>
					<tokenizers>
						<tokenizer>
							<identifier>basicWeiboTokenzier</identifier>
							<param>
								<lexer_identifier>basicUTF8Lexer</lexer_identifier>
								<token_filters>
									<token_filter>
										<identifier>defaultEnFilter</identifier>
										<param>
                                            <en_pos_senti_file>../data/dict/en_pos_senti.dic</en_pos_senti_file>
                                            <en_neg_senti_file>../data/dict/en_neg_senti.dic</en_neg_senti_file>
                                            <en_stopword_file>../data/dict/en_stop_word.dic</en_stopword_file>
											<stem>false</stem>
											<to_lowcase>true</to_lowcase>
										</param>
									</token_filter>
								</token_filters>
							</param>
						</tokenizer>
						<tokenizer>
							<identifier>chNGramTokenzier</identifier>
							<param>
								<lexer_identifier>basicUTF8Lexer</lexer_identifier>
								<max_n type=int>3</max_n>
								<token_filters>
									<token_filter>
										<identifier>defaultNgramFilter</identifier>
										<param>
                                            <ch_negation_word_file>../data/dict/cn_negation_word.dic</ch_negation_word_file>
										</param>
									</token_filter>
								</token_filters>
							</param>
						</tokenizer>
					</tokenizers>
				</analyzer>
			</text_analyzer>
		</param>
	</number_senti_classifier>
	<number_senti_classifier>
		<number_weight type=float>0.5</number_weight>
		<identifier>lexcion_senti_classifier</identifier>
		<param>
			<text_analyzer>
				<is_thread_safe>true</is_thread_safe>
				<analyzer>
					<encode>utf8</encode>
					<tokenizers>
						<tokenizer>
							<identifier>basicWeiboTokenzier</identifier>
							<param>
								<lexer_identifier>basicUTF8Lexer</lexer_identifier>
								<token_filters>
									<token_filter>
										<identifier>defaultEnFilter</identifier>
										<param>
                                            <en_pos_senti_file>../data/dict/en_pos_senti.dic</en_pos_senti_file>
                                            <en_neg_senti_file>../data/dict/en_neg_senti.dic</en_neg_senti_file>
                                            <en_stopword_file>../data/dict/en_stop_word.dic</en_stopword_file>
                                            <en_negation_word_file>../data/dict/en_negation_word.dic</en_negation_word_file>
											<stem>false</stem>
											<to_lowcase>true</to_lowcase>
										</param>
									</token_filter>
								</token_filters>
							</param>
						</tokenizer>
						<tokenizer>
							<identifier>complexMaxMatchTokenzier</identifier>
							<param>
								<lexer_identifier>basicUTF8Lexer</lexer_identifier>
                                <dict_file>../data/dict/std_dict.weibo.senticlf.dat</dict_file>
							</param>
						</tokenizer>
					</tokenizers>
				</analyzer>
			</text_analyzer>
			<lexcion_senti_scorer>
				<identifier>senti_label_senti_scorer</identifier>
				<param>
                    <senti_label_lexcion_file>../data/dict/weight_senti_dict.weibo.dat</senti_label_lexcion_file>
                    <after_only_degree_words_file>../data/dict/after_only_degree_word.dic</after_only_degree_words_file>
                    <before_only_degree_words_file>../data/dict/before_only_degree_word.dic</before_only_degree_words_file>
					<position_weighter>
						<position_aware>false</position_aware>
						<quota_penalty type=float>1</quota_penalty>
						<but_conj_boost type=float>1.5</but_conj_boost>
						<though_conj_penalty type=float>0.25</though_conj_penalty>
						<question_mark_penalty type=float>1</question_mark_penalty>
					</position_weighter>
					<senti_label_extractor>
						<identifier>simple_senti_label_extractor</identifier>
						<param>
                            <inter_words_file>../data/dict/cn_inter_word.dic</inter_words_file>
                            <senti_negation_words_file>../data/dict/senti_negation_word.dic</senti_negation_words_file>
                            <senti_degree_words_file>../data/dict/senti_degree_word.dic</senti_degree_words_file>
                            <negation_senti_words_file>../data/dict/negation_senti_word.dic</negation_senti_words_file>
						</param>
					</senti_label_extractor>
				</param>
			</lexcion_senti_scorer>
		</param>
	</number_senti_classifier>
</number_senti_classifiers>
